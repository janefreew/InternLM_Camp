# RAG 概述

RAG（Retrieval Augmented Generation）技术，通过==检索==与用户输入相关的信息片段，并结合***==外部知识库==***来生成更准确、更丰富的回答。解决 LLMs 在处理知识密集型任务时可能遇到的挑战, 如幻觉、知识过时和缺乏透明、可追溯的推理过程等。提供更准确的回答、降低推理成本、实现外部记忆。

- Indexing 索引
  - 将知识源(如文档或网页)分割成chunck ，编码成向量，并存储在向量数据库中

- VectorDB

- Retrieval 检索

  - 接收到用户的问题后，将问题也编码成向量，并在向量数据库中找到与之最相关的文档块(top-k chuncks)

- Generation 生成 

  - 将检索到的文档块与原始问题一起作为提示(prompt)， 输入到LLM中，生成最终的回答。

    

## 向量数据库(Vector-DB)

### 数据存储

- 将文本及其他数据通过其他预训练的模型转换为固定长度的向量表示，这些向量能够捕捉文本的语义信息。

### 相似性检索

- 点乘
- Cosine Similarity

### 向量表示的优化

- 使用更高级的文本编码技术
  - 句子嵌入
  - 段落嵌入
  - 对数据库进行优化以支持大规模向量搜索

## RAG 发展进程

## RAG 常见优化方法

- 嵌入优化

  - 结合稀疏和密集检索
  - 多任务

- 索引优化

  - 细粒度分割
  - 元数据

- 查询优化

  - 查询扩展 转换
  - 多查询

- 上下文管理

  - 重排 rereank
  - 上下文选择/压缩

- 迭代检索

  - 根据初始查询和迄今为止生成的文本进行重复搜索

- 递归检索

  - 迭代细化搜索查询
  - 链式推理指导索引过程

- 自适应检索

  - Flare , self-RAG 
  - 使用LLMs 主动决定检索的最佳时机和内容

- LLM 微调

  - 检索微调
  - 生成微调
  - 双重微调

  

# 茴香豆介绍

## 核心特性

- 开源免费
- 高效准确
  - Hybrid LLMs
  - 专为群聊优化
- 领域知识
  - 应用RAG 技术
  - 专业知识快速获取
- 部署成本低
  - 无需额外训练
  - 可利用云端模型api
  - 本地算力需求少
- 安全
  - 可安全本地部署
  - 信息不上传
  - 保护数据和用户隐私
- 扩展性强
  - 兼容多种IM 软件
  - 支持多种开源LLMs和云端API

## 茴香豆构建

### 知识库

- pdf 
- md
- .....

### 前端

### 后端

- 本地大模型支持

  - 书生浦语

  - 通义千问

- 远端模型

  - Kimi
  - chat-gpt
  - chatGLM

### 豆哥

## 工作流

![image-20240408155128639](https://gitee.com/janefreew/pic-bed/raw/master/img/image-20240408155128639.png)